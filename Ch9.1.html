<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>预训练阶段 - Robotic Embodied Intelligence - From Zero to Hero</title>


        <!-- Custom HTML head -->

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon-de23e50b.svg">
        <link rel="shortcut icon" href="favicon-8114d1fc.png">
        <link rel="stylesheet" href="css/variables-8adf115d.css">
        <link rel="stylesheet" href="css/general-2459343d.css">
        <link rel="stylesheet" href="css/chrome-ae938929.css">
        <link rel="stylesheet" href="css/print-9e4910d8.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="fonts/fonts-9644e21d.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="mdbook-highlight-css" href="highlight-493f70e1.css">
        <link rel="stylesheet" id="mdbook-tomorrow-night-css" href="tomorrow-night-4c0ae647.css">
        <link rel="stylesheet" id="mdbook-ayu-highlight-css" href="ayu-highlight-3fdfc3ac.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="src/css/custom-a0f5ac24.css">

        <!-- MathJax -->
        <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "";
            const default_light_theme = "light";
            const default_dark_theme = "navy";
            window.path_to_searchindex_js = "searchindex-9218460c.js";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="toc-a5a73760.js"></script>
    </head>
    <body>
    <div id="mdbook-help-container">
        <div id="mdbook-help-popup">
            <h2 class="mdbook-help-title">Keyboard shortcuts</h2>
            <div>
                <p>Press <kbd>←</kbd> or <kbd>→</kbd> to navigate between chapters</p>
                <p>Press <kbd>S</kbd> or <kbd>/</kbd> to search in the book</p>
                <p>Press <kbd>?</kbd> to show this help</p>
                <p>Press <kbd>Esc</kbd> to hide this help</p>
            </div>
        </div>
    </div>
    <div id="mdbook-body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="mdbook-sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("mdbook-sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
                sidebar_toggle.checked = false;
            }
            if (sidebar === 'visible') {
                sidebar_toggle.checked = true;
            } else {
                html.classList.remove('sidebar-visible');
            }
        </script>

        <nav id="mdbook-sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="toc.html"></iframe>
            </noscript>
            <div id="mdbook-sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="mdbook-page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="mdbook-menu-bar-hover-placeholder"></div>
                <div id="mdbook-menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="mdbook-sidebar-toggle" class="icon-button" for="mdbook-sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="mdbook-sidebar">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"/></svg></span>
                        </label>
                        <button id="mdbook-theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="mdbook-theme-list">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M371.3 367.1c27.3-3.9 51.9-19.4 67.2-42.9L600.2 74.1c12.6-19.5 9.4-45.3-7.6-61.2S549.7-4.4 531.1 9.6L294.4 187.2c-24 18-38.2 46.1-38.4 76.1L371.3 367.1zm-19.6 25.4l-116-104.4C175.9 290.3 128 339.6 128 400c0 3.9 .2 7.8 .6 11.6c1.8 17.5-10.2 36.4-27.8 36.4H96c-17.7 0-32 14.3-32 32s14.3 32 32 32H240c61.9 0 112-50.1 112-112c0-2.5-.1-5-.2-7.5z"/></svg></span>
                        </button>
                        <ul id="mdbook-theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-ayu">Ayu</button></li>
                        </ul>
                        <button id="mdbook-search-toggle" class="icon-button" type="button" title="Search (`/`)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="/ s" aria-controls="mdbook-searchbar">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M416 208c0 45.9-14.9 88.3-40 122.7L502.6 457.4c12.5 12.5 12.5 32.8 0 45.3s-32.8 12.5-45.3 0L330.7 376c-34.4 25.2-76.8 40-122.7 40C93.1 416 0 322.9 0 208S93.1 0 208 0S416 93.1 416 208zM208 352c79.5 0 144-64.5 144-144s-64.5-144-144-144S64 128.5 64 208s64.5 144 144 144z"/></svg></span>
                        </button>
                    </div>

                    <h1 class="menu-title">Robotic Embodied Intelligence - From Zero to Hero</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <span class=fa-svg id="print-button"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M128 0C92.7 0 64 28.7 64 64v96h64V64H354.7L384 93.3V160h64V93.3c0-17-6.7-33.3-18.7-45.3L400 18.7C388 6.7 371.7 0 354.7 0H128zM384 352v32 64H128V384 368 352H384zm64 32h32c17.7 0 32-14.3 32-32V256c0-35.3-28.7-64-64-64H64c-35.3 0-64 28.7-64 64v96c0 17.7 14.3 32 32 32H64v64c0 35.3 28.7 64 64 64H384c35.3 0 64-28.7 64-64V384zm-16-88c-13.3 0-24-10.7-24-24s10.7-24 24-24s24 10.7 24 24s-10.7 24-24 24z"/></svg></span>
                        </a>
                        <a href="https://Yang-Yang.me/" title="Git repository" aria-label="Git repository">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M272 304h-96C78.8 304 0 382.8 0 480c0 17.67 14.33 32 32 32h384c17.67 0 32-14.33 32-32C448 382.8 369.2 304 272 304zM48.99 464C56.89 400.9 110.8 352 176 352h96c65.16 0 119.1 48.95 127 112H48.99zM224 256c70.69 0 128-57.31 128-128c0-70.69-57.31-128-128-128S96 57.31 96 128C96 198.7 153.3 256 224 256zM224 48c44.11 0 80 35.89 80 80c0 44.11-35.89 80-80 80S144 172.1 144 128C144 83.89 179.9 48 224 48z"/></svg></span>
                        </a>

                    </div>
                </div>

                <div id="mdbook-search-wrapper" class="hidden">
                    <form id="mdbook-searchbar-outer" class="searchbar-outer">
                        <div class="search-wrapper">
                            <input type="search" id="mdbook-searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="mdbook-searchresults-outer" aria-describedby="searchresults-header">
                            <div class="spinner-wrapper">
                                <span class=fa-svg id="fa-spin"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M304 48c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zm0 416c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zM48 304c26.5 0 48-21.5 48-48s-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48zm464-48c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zM142.9 437c18.7-18.7 18.7-49.1 0-67.9s-49.1-18.7-67.9 0s-18.7 49.1 0 67.9s49.1 18.7 67.9 0zm0-294.2c18.7-18.7 18.7-49.1 0-67.9S93.7 56.2 75 75s-18.7 49.1 0 67.9s49.1 18.7 67.9 0zM369.1 437c18.7 18.7 49.1 18.7 67.9 0s18.7-49.1 0-67.9s-49.1-18.7-67.9 0s-18.7 49.1 0 67.9z"/></svg></span>
                            </div>
                        </div>
                    </form>
                    <div id="mdbook-searchresults-outer" class="searchresults-outer hidden">
                        <div id="mdbook-searchresults-header" class="searchresults-header"></div>
                        <ul id="mdbook-searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('mdbook-sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('mdbook-sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#mdbook-sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="mdbook-content" class="content">
                    <main>
                        <h3 id="91-预训练阶段概述承上启下小结"><a class="header" href="#91-预训练阶段概述承上启下小结">9.1 预训练阶段概述（承上启下小结）</a></h3>
<p>在上一章我们主要从结构上讨论了 VLA 模型“长什么样”。从本节开始，我们转向一个更工程化的问题：<strong>如何让这样一个模型“有东西可用”</strong>——也就是预训练阶段。</p>
<p>由于机器人真实交互数据昂贵、危险且难以规模化，当前主流做法是先依托互联网海量图文数据，把模型训练成一个强大的视觉–语言基础模型，再在机器人数据上做模仿学习和 RL 微调。(<a href="https://arxiv.org/abs/2103.00020?utm_source=chatgpt.com">arXiv</a>)</p>
<p>本小节聚焦四个问题：</p>
<ol>
<li>用什么样的互联网图文语料来做预训练？</li>
<li>视觉 backbone 和语言 backbone 应该如何初始化？</li>
<li>自监督 / 半监督任务如何提升预训练效果？</li>
<li>如何在多任务预训练中合理设计和加权损失？</li>
</ol>
<hr>
<h3 id="911-利用互联网图文数据进行视觉语言预训练"><a class="header" href="#911-利用互联网图文数据进行视觉语言预训练">9.1.1 利用互联网图文数据进行视觉–语言预训练</a></h3>
<h4 id="9111-图文语料"><a class="header" href="#9111-图文语料">9.1.1.1 图文语料</a></h4>
<p><strong>1）互联网图文数据的来源形式</strong></p>
<p>互联网提供了大规模的“天然多模态数据”：</p>
<ul>
<li>图片 + alt 文本（HTML 中的 alt 属性）</li>
<li>图片 + 标题 / 正文片段（新闻、博客、商品页面）</li>
<li>图片 + 用户评论（社交媒体）</li>
<li>图片 + 自动生成的弱描述（如搜索引擎爬虫产生的元数据）</li>
</ul>
<p>早期数据集如 <strong>Conceptual Captions</strong>、<strong>YFCC100M</strong> 等，就是从网页中抽取“图片 + 描述文字”对，经过一定规则过滤构建而成。ALIGN 工作沿用了类似流程，但减少了清洗程度，用极大规模弥补噪声问题。(<a href="https://arxiv.org/abs/2102.05918?utm_source=chatgpt.com">arXiv</a>)</p>
<p>近几年，<strong>LAION 系列数据集</strong>将这一思路推向了“互联网级”规模：</p>
<ul>
<li><strong>LAION-400M</strong>：约 4 亿图文对，通过 CLIP 过滤保证基本语义相关性。(<a href="https://arxiv.org/abs/2111.02114?utm_source=chatgpt.com">arXiv</a>)</li>
<li><strong>LAION-5B</strong>：扩展到约 58 亿多语种图文对，是目前公开可用的最大多模态数据集之一。(<a href="https://laion.ai/laion-5b-a-new-era-of-open-large-scale-multi-modal-datasets/?utm_source=chatgpt.com">laion.ai</a>)</li>
</ul>
<p>这些数据集的共同特点：</p>
<ul>
<li><strong>规模极大</strong>：使模型可以在非常多样的场景中看到各种物体、动作、风格。</li>
<li><strong>标注极弱且含噪</strong>：文本并不是精心写的“图像描述”，而往往只是上下文碎片。</li>
<li><strong>覆盖面广</strong>：从生活照片到截图、表格、梗图、漫画，应有尽有。</li>
</ul>
<p>对机器人而言，这类数据为“理解世界长什么样”提供了极其丰富的先验，为之后的具身学习打下语义基底。</p>
<p><strong>2）数据清洗与安全过滤</strong></p>
<p>大规模网络语料必然包含：</p>
<ul>
<li>不相关的图文对（图和文完全不匹配）</li>
<li>噪声文字（乱码、广告、SEO 垃圾）</li>
<li>不适宜内容（成人、暴力、隐私等）</li>
</ul>
<p>典型清洗步骤包括：</p>
<ul>
<li>文本层面：去掉过短或过长的文本，过滤非自然语言、广告模板等。</li>
<li>图像层面：约束分辨率范围，去掉纯色图、损坏图。</li>
<li>语义相关性过滤：用一个已训练好的 CLIP 模型，对图文对打分，只保留相似度超过阈值的样本（LAION 系列采用此类方法）。(<a href="https://arxiv.org/abs/2111.02114?utm_source=chatgpt.com">arXiv</a>)</li>
<li>安全过滤：利用专门模型过滤 NSFW、暴力或隐私内容。</li>
</ul>
<blockquote>
<p>【图 9-1 占位：互联网图文数据预处理流程示意图。从“原始网页抓取”到“图像/文本预筛选”“CLIP 相似度过滤”“安全过滤”，最后得到干净的图文对。】</p>
</blockquote>
<p>在面向机器人应用时，还可以增加额外的“领域过滤”，例如优先保留包含室内场景、桌面物体、工具、人体日常动作等图像，以提高对后续操作任务的相关性。</p>
<h4 id="9112-预训练任务"><a class="header" href="#9112-预训练任务">9.1.1.2 预训练任务</a></h4>
<p>在具备大规模图文语料后，关键问题是：<strong>用什么任务来驱动模型学习有用的视觉–语言特征？</strong></p>
<p>当前主流预训练目标大致分为三类：对比式、匹配式和生成式。</p>
<p><strong>1）对比式预训练：CLIP / ALIGN / LiT</strong></p>
<p>典型代表是 OpenAI 的 <strong>CLIP（Contrastive Language–Image Pre-training）</strong> 和 Google 的 <strong>ALIGN</strong>、<strong>LiT</strong> 等。(<a href="https://arxiv.org/abs/2103.00020?utm_source=chatgpt.com">arXiv</a>)</p>
<p>核心思想是：</p>
<ul>
<li>使用一个图像编码器 (f_\text{img}) 和一个文本编码器 (f_\text{text})。</li>
<li>对于一个 batch 中的 (N) 个图文对 ((I_i, T_i))，分别编码得到向量 (\mathbf{v}<em>i = f</em>\text{img}(I_i))，(\mathbf{u}<em>i = f</em>\text{text}(T_i))。</li>
<li>将匹配的 ((I_i, T_i)) 作为<strong>正样本</strong>，不匹配的组合作为<strong>负样本</strong>。</li>
<li>使用 InfoNCE 形式的对比损失：
$$
\mathcal{L}_\text{img2text}
= - \frac{1}{N} \sum_i
\log \frac{\exp(\text{sim}(\mathbf{v}_i,\mathbf{u}_i)/\tau)}
{\sum_j \exp(\text{sim}(\mathbf{v}_i,\mathbf{u}_j)/\tau)}
$$
文本到图像方向类似，(\text{sim}) 通常为向量点积，(\tau) 为温度系数。</li>
</ul>
<p>直观理解：模型被迫将<strong>语义上相关的图像和文本“拉近”到同一个向量空间</strong>，而把不相关的推远。训练完成后，我们可以：</p>
<ul>
<li>用一句文本在图像库中检索匹配图片（文本→图像）</li>
<li>用一张图像在文本库中检索匹配描述（图像→文本）</li>
<li>甚至做零样本分类：将类别名称转为文本，编码后与图像特征做相似度比较。(<a href="https://arxiv.org/abs/2103.00020?utm_source=chatgpt.com">arXiv</a>)</li>
</ul>
<p>ALIGN 则证明，即便图文配对噪声较大，只要数据规模足够大，对比学习仍然能够学到非常强的视觉表示。(<a href="https://arxiv.org/abs/2102.05918?utm_source=chatgpt.com">arXiv</a>)</p>
<p>LiT（Locked-image Text Tuning）进一步提出：<strong>锁死一个强大的预训练图像模型，只训练文本侧</strong>，用对比目标将文本 embedding 对齐到图像 embedding 空间中，从而极高效地获得强大的图文模型。(<a href="https://arxiv.org/abs/2111.07991?utm_source=chatgpt.com">arXiv</a>)
这一思路在 9.1.2 节会再次出现。</p>
<blockquote>
<p>【图 9-2 占位：CLIP/ALIGN 双塔结构示意图。左侧为图像编码器，右侧为文本编码器，中间用对比损失对齐。】</p>
</blockquote>
<p><strong>2）匹配式预训练：图文匹配（ITM）</strong></p>
<p>除了软对比损失，还可以显式训练一个二分类任务：给定一对 ((I, T))，预测它们是否匹配（Image–Text Matching, ITM）。许多单流 VLM（如 ViLBERT、UNITER 等）都使用类似目标。</p>
<p>优点：</p>
<ul>
<li>学习更细粒度的对齐（部分错误描述也会被判为“不匹配”）。</li>
<li>匹配得分可以直接用作检索排序或后续模块的置信度。</li>
</ul>
<p><strong>3）生成式预训练：BLIP 等统一理解–生成模型</strong></p>
<p>对机器人尤其重要的一类目标，是<strong>从图像生成自然语言描述</strong>，以及反过来<strong>从文本生成与图像一致的语言/特征</strong>。</p>
<p>BLIP（Bootstrapping Language-Image Pre-training）提出了一个统一的 VLP 框架，同时支持理解和生成任务：它使用多模态编码–解码架构，联合优化图文对比、图文匹配和语言建模等多个目标，并通过“生成新 caption + 过滤噪声 caption”的方式提升 Web 图文数据质量。(<a href="https://arxiv.org/abs/2201.12086?utm_source=chatgpt.com">arXiv</a>)</p>
<p>对于机器人而言，这类生成能力可以用于：</p>
<ul>
<li>让模型<strong>给自己看到的场景配上语言描述</strong>，帮助理解操作环境；</li>
<li>利用语言模型对机器人执行过程进行解释（第 4.4.3 节会展开）。</li>
</ul>
<h4 id="9113-通用语义特征"><a class="header" href="#9113-通用语义特征">9.1.1.3 通用语义特征</a></h4>
<p>对比式 / 匹配式 / 生成式预训练的共同目标，是获得一个 <strong>“通用视觉–语言语义空间”</strong>：</p>
<ul>
<li>空间中的每个向量既有“视觉含义”，又对应“语言含义”；</li>
<li>相似向量对应相近的语义概念（如“杯子”“coffee mug”“水杯”会聚在一起）。</li>
</ul>
<p>CLIP 类模型已经展示出强大的零样本迁移能力：在未见过标注的分类任务上，仅通过语言描述类别名称，就能接近甚至匹敌监督训练的 ResNet-50。(<a href="https://arxiv.org/abs/2103.00020?utm_source=chatgpt.com">arXiv</a>)</p>
<p>ALIGN、LiT 等工作证明，<strong>只要视觉 backbone 足够强、图文对齐做得好，这一语义空间即可广泛迁移到下游任务</strong>。(<a href="https://arxiv.org/abs/2102.05918?utm_source=chatgpt.com">arXiv</a>)</p>
<p>对具身智能而言，这一通用语义空间有几种直接用途：</p>
<ol>
<li><strong>作为视觉 encoder 初始化</strong>：将机器人摄像头图像送入预训练的图像编码器，得到语义丰富的特征，再供下游决策网络使用。</li>
<li><strong>作为语言条件接口</strong>：用户的自然语言指令编码后与视觉特征处于同一空间，方便 VLA 模型进行“以文找物”的对齐（第 8.4 节）。</li>
<li><strong>作为跨任务共享表示</strong>：同一语义空间既可以支持分类/检测，也可以支持操作策略学习，避免为每类任务单独训练视觉模型。</li>
</ol>
<blockquote>
<p>【图 9-3 占位：通用图文语义空间示意图。不同类别图像与对应文本在高维空间中形成簇，机器人操作任务在该空间中选取目标对象。】</p>
</blockquote>
<hr>
<h3 id="912-视觉-backbone-与语言-backbone-的初始化策略"><a class="header" href="#912-视觉-backbone-与语言-backbone-的初始化策略">9.1.2 视觉 backbone 与语言 backbone 的初始化策略</a></h3>
<p>真实机器人项目通常<strong>不会从随机初始化训练一个 VLA 模型</strong>，而是高度依赖预训练好的视觉 / 语言 backbone。合理的初始化策略可以显著降低数据需求、提高收敛速度。</p>
<h4 id="9121-视觉模型初始化"><a class="header" href="#9121-视觉模型初始化">9.1.2.1 视觉模型初始化</a></h4>
<p>可以大致分为三种来源：</p>
<p><strong>1）纯视觉预训练模型</strong></p>
<ul>
<li><strong>监督预训练</strong>：如在 ImageNet 上监督训练的 ResNet、ViT 等。</li>
<li><strong>自监督预训练</strong>：如对比学习（SimCLR、MoCo、DINO）、掩码图像建模（MAE、BEiT 等）。MAE 通过随机遮挡高比例图像 Patch，并让模型重建被遮挡区域，学习到泛化性很强的视觉特征。(<a href="https://arxiv.org/abs/2111.06377?utm_source=chatgpt.com">arXiv</a>)</li>
</ul>
<p>优点：</p>
<ul>
<li>表征偏向纯几何/纹理/语义信息，对语言分布不过度依赖；</li>
<li>适合作为“中性”的视觉基石，再与语言对齐（如 LiT 的做法）。</li>
</ul>
<p><strong>2）视觉–语言联合预训练的视觉 encoder</strong></p>
<p>CLIP、ALIGN、BLIP 等模型内置的视觉编码器，已经在图文任务上做过对齐训练，通常更擅长“对着语言找对应区域”。</p>
<p>例如：</p>
<ul>
<li>使用 CLIP-ViT 作为视觉 backbone，可以直接获得与文本空间对齐的特征，方便后续指令条件控制。(<a href="https://arxiv.org/abs/2103.00020?utm_source=chatgpt.com">arXiv</a>)</li>
<li>InternVL 等近期大模型则从头联合训练大规模视觉 encoder 和 LLM，使视觉 backbone 更适配下游多模态任务。(<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_InternVL_Scaling_up_Vision_Foundation_Models_and_Aligning_for_Generic_CVPR_2024_paper.pdf?utm_source=chatgpt.com">CVF开放获取</a>)</li>
</ul>
<p><strong>3）如何为机器人场景做选择？</strong></p>
<p>实践中常见的几种组合：</p>
<ul>
<li><strong>桌面操作 / 室内服务机器人</strong>：环境与互联网图片相似度较高，可优先选择 CLIP / BLIP 类视觉 backbone。</li>
<li><strong>专业领域（手术机器人、工业检查等）</strong>：可以先用自监督方法在领域内的无标注图像上做额外预训练，再与语言对齐。</li>
<li><strong>资源受限平台</strong>：可能需要从较小的 CNN/ViT 结构开始，再通过知识蒸馏等手段迁移大模型的表示（见 10.4 节）。</li>
</ul>
<h4 id="9122-语言模型初始化"><a class="header" href="#9122-语言模型初始化">9.1.2.2 语言模型初始化</a></h4>
<p>语言侧的选择更加多样，大致可以分为两类：</p>
<p><strong>1）轻量级文本编码器</strong></p>
<p>如 BERT / RoBERTa 风格的编码器，输入为子词 token 序列，输出句子 embedding 或每个 token 的 contextual 表示。特点：</p>
<ul>
<li>模型尺寸中等，适合嵌入式部署；</li>
<li>适合作为“指令编码器”，将任务描述转为固定维向量，供决策模块条件使用。</li>
</ul>
<p>这类模型通常已经在大规模文本语料上预训练，具备较强的语义理解能力，对于“把红色方块放进盒子”这类简单指令完全够用。</p>
<p><strong>2）大语言模型（LLM）作为语言 backbone</strong></p>
<p>更激进的路线是直接使用 GPT/T5/LLAMA 级别的大语言模型作为语言 backbone：</p>
<ul>
<li>优点：具备丰富世界知识和推理能力，可对任务指令做更复杂的改写、分解和解释。</li>
<li>缺点：推理开销较大，延迟和内存压力重，对实时控制是挑战。</li>
</ul>
<p>许多最新 VLM / VLA 工作的做法是：<strong>冻结大部分 LLM 参数，只在其输入侧或中间插入少量可训练模块（如 LoRA、Adapter）</strong>，在保证语言能力的前提下调节其对视觉和动作信号的响应（第 2.5.3 节已介绍参数高效微调思想，这里不再赘述）。</p>
<h4 id="9123-冻结-vs-微调"><a class="header" href="#9123-冻结-vs-微调">9.1.2.3 冻结 vs 微调</a></h4>
<p>初始化只是第一步，接下来要决定：<strong>这些预训练的 backbone，要不要继续训练？如何训练？</strong></p>
<p>可以从以下几个层次理解：</p>
<p><strong>1）完全冻结（全冻结）</strong></p>
<p>做法：</p>
<ul>
<li>视觉 encoder 和文本 encoder 完全不更新；</li>
<li>仅在其上方训练一个较小的多模态融合 + 动作解码头。</li>
</ul>
<p>优点：</p>
<ul>
<li>最稳定、安全，不会破坏预训练好的通用能力；</li>
<li>计算和显存开销小，适合数据非常少的场景。</li>
</ul>
<p>缺点：</p>
<ul>
<li>对特定机器人视角、噪声形态、指令风格的适应能力有限；</li>
<li>对具有明显领域偏移的场景（工业、医疗）效果可能较差。</li>
</ul>
<p>LiT 的结果表明，在某些情况下，“锁死图像模型，只训练文本侧”可以达到很强的零样本性能，这对机器人提示我们：<strong>合理的冻结策略本身就是一种强有力的先验</strong>。(<a href="https://arxiv.org/abs/2111.07991?utm_source=chatgpt.com">arXiv</a>)</p>
<p><strong>2）部分微调 / 分层解冻</strong></p>
<ul>
<li>只微调 backbone 的高层（靠近输出的几层），保留底层边缘/纹理特征；</li>
<li>或者采用“逐层解冻”（逐步解冻）：先只训练新增头部，稳定后再逐层解冻 backbone 深层。</li>
</ul>
<p>优点：在保持大体分布的同时，让模型适应机器人视角、相机畸变、特定语言风格。</p>
<p><strong>3）参数高效微调（PEFT）</strong></p>
<p>如 LoRA、Adapter、Prefix Tuning 等，只在 backbone 内部插入小规模可训练参数。优点：</p>
<ul>
<li>几乎不改变原有权重，避免灾难性遗忘；</li>
<li>可以为不同机器人 / 任务维护多套“适配器”，共用同一个基础模型。</li>
</ul>
<p>在机器人 VLA 中常见做法是：</p>
<ul>
<li>冻结大部分 CLIP/LLM 参数；</li>
<li>在跨模态对齐层或动作解码层插入 LoRA/Adapter；</li>
<li>使用机器人多模态数据训练这些轻量参数。</li>
</ul>
<p><strong>4）如何选择策略？一个实用经验</strong></p>
<ul>
<li>机器人数据极少（几十到几百条演示）：倾向<strong>冻结 backbone + 仅训练头部或 PEFT</strong>。</li>
<li>有中等规模数据（上万条演示）：可以<strong>部分微调高层</strong>，尤其是靠近动作解码器的一侧。</li>
<li>仿真可生成极大规模数据：可以尝试<strong>全模型微调</strong>，甚至在仿真中从强初始化开始继续预训练。</li>
</ul>
<hr>
<h3 id="913-自监督--半监督任务在预训练中的作用"><a class="header" href="#913-自监督--半监督任务在预训练中的作用">9.1.3 自监督 / 半监督任务在预训练中的作用</a></h3>
<p>互联网图文预训练提供的是**“人类视角的语义先验”**，而机器人自身的传感数据（机器人的第一人称视角、低机位、特定传感器噪声等）往往分布不同。</p>
<p>为了充分利用大量 <strong>无标注或弱标注的机器人数据</strong>，自监督与半监督任务成为连接互联网预训练与具身学习的重要桥梁。</p>
<h4 id="9131-掩码建模"><a class="header" href="#9131-掩码建模">9.1.3.1 掩码建模</a></h4>
<p>掩码建模（Masked Modeling）是一类非常通用的自监督方法，包含两种典型形式：</p>
<p><strong>1）掩码语言建模（MLM）</strong></p>
<ul>
<li>在输入句子中随机遮盖部分 token（例如用 <code>[MASK]</code> 替代）；</li>
<li>训练模型根据上下文预测被遮盖 token；</li>
<li>BERT 系列模型就是通过 MLM 预训练的代表。</li>
</ul>
<p>对机器人而言，MLM 可用于让模型熟悉<strong>任务描述的语言模式</strong>，并强化对动作说明、约束条件等关键字的敏感度。</p>
<p><strong>2）掩码图像建模（MIM）：以 MAE 为代表</strong></p>
<p>MAE（Masked Autoencoders）将掩码思想引入图像：</p>
<ul>
<li>将图像分割为多个 Patch，随机遮挡其中大部分（例如 75%）；</li>
<li>编码器只处理可见 Patch，解码器从潜在表示 + 掩码 token 中重建完整图像；(<a href="https://arxiv.org/abs/2111.06377?utm_source=chatgpt.com">arXiv</a>)</li>
<li>通过重建误差作为训练信号。</li>
</ul>
<p>这类方法的直觉是：
要填补被遮挡区域，模型必须学会图像中物体的<strong>结构与上下文关系</strong>，而不是仅仅记住局部纹理。</p>
<p>进一步地，还有同时对图像和文本做掩码建模的多模态 MAE（如 M3AE 等），在 awesome-VLP 汇总中可以找到一系列工作，它们利用多模态掩码任务学习可迁移的视觉–语言表示。(<a href="https://github.com/fawazsammani/awesome-vision-language-pretraining?utm_source=chatgpt.com">GitHub</a>)</p>
<p>在机器人场景中，可以将<strong>机器人第一视角摄像头的连续画面</strong>作为 MIM 的训练对象，让模型更好地适应真实硬件和环境下的视觉分布，为后续 VLA 预训练提供“贴地气”的视觉基座。</p>
<blockquote>
<p>【图 9-4 占位：多模态掩码建模示意图。一侧是被遮挡 Patch 的图像，另一侧是遮挡 token 的指令文本，解码器联合重建。】</p>
</blockquote>
<h4 id="9132-顺序预测"><a class="header" href="#9132-顺序预测">9.1.3.2 顺序预测</a></h4>
<p>具身智能本质上是<strong>时序决策问题</strong>，因此“顺序预测”（Sequential Prediction）类自监督任务与机器人高度契合。</p>
<p>常见形式包括：</p>
<p><strong>1）语言序列的下一个 token 预测</strong></p>
<p>这是自回归语言模型（GPT 类）的经典预训练目标：给定前面一串 token，预测下一个 token。它让模型习得语法、知识及一定程度的推理能力（第 4.2 节已详细介绍）。</p>
<p><strong>2）视觉 / 视频序列预测</strong></p>
<ul>
<li>未来帧预测（Future Frame Prediction）：给定过去几帧图像，预测下一帧图像或其特征；</li>
<li>视频片段顺序判别：打乱一段视频的帧顺序，让模型判断是否在正确顺序或恢复正确顺序。</li>
</ul>
<p>对机器人而言，这类任务相当于训练一个粗略的“世界模型”：<strong>在不执行真实动作的情况下，想象环境会怎样变化</strong>。</p>
<p><strong>3）动作 / 轨迹序列预测</strong></p>
<p>在拥有大量机器人交互轨迹（甚至无任务标签）的情况下，可以构造纯自监督任务：</p>
<ul>
<li>给定过去若干步的观察和动作，预测下一步动作（或状态）；</li>
<li>给定轨迹的前半段，预测后半段动作 / 状态。</li>
</ul>
<p>这些任务不需要外部奖励或成功标注，但能让模型学习到<strong>动作–状态之间的因果结构</strong>，后续行为克隆或 RL 微调会更高效。</p>
<h4 id="9133-半监督学习"><a class="header" href="#9133-半监督学习">9.1.3.3 半监督学习</a></h4>
<p>在许多领域，<strong>图像远多于配对文本</strong>，或者只有少量专家标注的图文或轨迹。半监督学习的目标是：</p>
<blockquote>
<p>利用少量标注 + 大量未标注数据，获得接近全监督的效果。</p>
</blockquote>
<p>在视觉–语言预训练中，典型例子包括：</p>
<p><strong>1）S-CLIP：少量 caption + 大量无标注图像</strong></p>
<p>S-CLIP 针对专业领域（遥感、时尚、科学图表等）提出了一种半监督 CLIP 训练方法：</p>
<ul>
<li>有少量带 caption 的图像，以及大量没有 caption 的图像；</li>
<li>通过最优传输和部分标签学习等策略，为未标注图像生成“伪 caption”或关键词；</li>
<li>结合对比学习和伪标签训练，大幅提升目标领域的零样本性能。(<a href="https://arxiv.org/abs/2305.14095?utm_source=chatgpt.com">arXiv</a>)</li>
</ul>
<p>这一思路可以直接迁移到机器人：
例如，在一个工厂或家庭环境中，只为少数场景人工编写多模态说明，其余大量相似场景通过 VLM 自动生成伪标签，作为额外训练信号。</p>
<p><strong>2）VLM 辅助伪标注：BLIP 的 Caption Bootstrapping</strong></p>
<p>BLIP 提出用已经训练好的 captioner 为 Web 图像生成新描述，并利用过滤器剔除明显不可信的 caption，从而在“噪声 Web 文本”之上**“自举”出更干净的监督信号**。(<a href="https://arxiv.org/abs/2201.12086?utm_source=chatgpt.com">arXiv</a>)</p>
<p>在机器人数据上可以采取类似做法：</p>
<ul>
<li>用预训练 VLM / LLM 自动为机器人轨迹打上语言标签（任务描述、错误原因等）；</li>
<li>使用人工审核少部分样本，训练一个过滤模型；</li>
<li>将高置信度伪标注加入训练，充当“廉价的弱监督”。</li>
</ul>
<p><strong>3）一致性正则与教师–学生模型</strong></p>
<p>与图像分类中的半监督类似，VLA 预训练也可以引入：</p>
<ul>
<li>对未标注样本做多种数据增强（不同视角、裁剪、颜色变换等），要求模型输出的一致性；</li>
<li>使用 teacher–student 架构（例如 EMA 均值教师），teacher 生成伪标签，student 学习，并不断更新 teacher。</li>
</ul>
<p>总的来说，自监督 / 半监督任务使我们能充分压榨<strong>机器人摄像头全天候采集的海量未标注数据</strong>，在不增加太多标注成本的情况下显著增强预训练效果。</p>
<hr>
<h3 id="914-多任务预训练与损失加权"><a class="header" href="#914-多任务预训练与损失加权">9.1.4 多任务预训练与损失加权</a></h3>
<p>到目前为止，我们分别介绍了对比学习、掩码建模、顺序预测、半监督等任务。实际系统中，往往不会只训练一个目标，而是<strong>在同一个模型上并行优化多个任务</strong>，形成多任务预训练框架。</p>
<p>这带来两个问题：</p>
<ol>
<li>如何设计多任务结构？</li>
<li>多个损失之间如何加权？</li>
</ol>
<h4 id="9141-多任务预训练"><a class="header" href="#9141-多任务预训练">9.1.4.1 多任务预训练</a></h4>
<p>多任务预训练（Multi-Task Pre-training）的基本思路是：<strong>共享一个主干网络，并在其上附着多个任务头，同时优化多种损失。</strong></p>
<p>以 BLIP 为例，它在一个多模态编码–解码架构上同时优化：(<a href="https://arxiv.org/abs/2201.12086?utm_source=chatgpt.com">arXiv</a>)</p>
<ul>
<li>图文对比损失（Image–Text Contrastive, ITC）</li>
<li>图文匹配损失（Image–Text Matching, ITM）</li>
<li>语言建模损失（LM）</li>
</ul>
<p>部分工作还会加入：</p>
<ul>
<li>掩码语言建模（MLM）或掩码多模态建模（M3AE、SIMLA 等）；</li>
<li>视觉侧的分类或检测辅助任务；</li>
<li>动作预测或行为克隆损失（在已经拥有机器人轨迹数据时）。</li>
</ul>
<blockquote>
<p>【图 9-5 占位：多任务预训练结构示意图。底部为共享视觉–语言编码器，顶部挂接 ITC、ITM、MLM、动作预测等多个任务头。】</p>
</blockquote>
<p>对于 VLA 预训练，一个典型的多任务组合可能是：</p>
<ul>
<li>任务 A：图文对比，学习通用图文对齐；</li>
<li>任务 B：掩码建模（图像 / 文本 / 视频），学习局部结构和长程依赖；</li>
<li>任务 C：行为克隆预训练，输入（图像，指令，历史动作），预测下一步动作；</li>
<li>任务 D：离线 RL 或奖励预测（如果有奖励信号）。</li>
</ul>
<p>通过共享 backbone，模型在优化任务 C/D 时可以同时利用任务 A/B 学到的<strong>概念和物理直觉</strong>；反过来，任务 A/B 的泛化能力也会因接触“动作和后果”的数据而增强。</p>
<h4 id="9142-损失函数加权"><a class="header" href="#9142-损失函数加权">9.1.4.2 损失函数加权</a></h4>
<p>多任务预训练中的难点在于：<strong>不同任务的损失尺度、收敛速度、难度都不同</strong>，如果简单地把它们相加，可能导致：</p>
<ul>
<li>某个损失主导梯度更新，其他任务几乎“学不到东西”；</li>
<li>有的损失指数级下降，有的基本不动，训练不稳定。</li>
</ul>
<p>常见的损失加权策略包括：</p>
<p><strong>1）手工设定固定权重</strong></p>
<p>最朴素也最常见的做法：
$$
\mathcal{L}<em>\text{total}
= \lambda_1 \mathcal{L}</em>\text{ITC}</p>
<ul>
<li>\lambda_2 \mathcal{L}_\text{ITM}</li>
<li>\lambda_3 \mathcal{L}_\text{MLM}</li>
<li>\lambda_4 \mathcal{L}_\text{BC}</li>
<li>\cdots
$$</li>
</ul>
<p>优点是实现简单；缺点是需要大量实验调参，且不同数据规模、模型大小下最佳权重不同。</p>
<p><strong>2）基于不确定性的动态加权（Kendall 等）</strong></p>
<p>Kendall 等提出了一种基于<strong>任务 homoscedastic 不确定性</strong>的加权方式，把不同任务的损失缩放系数视为需要学习的参数：(<a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Kendall_Multi-Task_Learning_Using_CVPR_2018_paper.pdf?utm_source=chatgpt.com">CVF开放获取</a>)</p>
<p>对任务 (i) 的损失 (\mathcal{L}<em>i)，总损失可写为
$$
\mathcal{L}</em>\text{total}
= \sum_i \left( \frac{1}{2\sigma_i^2} \mathcal{L}_i + \log\sigma_i \right)
$$
其中 (\sigma_i) 是可学习的“任务不确定性”，训练过程中会自动调整，使得：</p>
<ul>
<li>损失较大的任务（噪声大或难学）获得相对较小权重；</li>
<li>损失较小、易优化的任务权重相对更大。</li>
</ul>
<p>这一方法的优势在于：</p>
<ul>
<li>不需要手动指定各任务权重；</li>
<li>可以自适应不同任务在不同训练阶段的重要性。</li>
</ul>
<p><strong>3）梯度平衡与学习进度驱动的加权</strong></p>
<p>其他方法（如 GradNorm、基于任务学习进度的动态权重等）通过平衡不同任务的梯度范数或收敛速度，让每个任务都能“公平地”参与训练。</p>
<p>在 VLA 预训练中，常见的实际工作流是：</p>
<ol>
<li>先采用简单的固定权重，快速跑通训练流水线；</li>
<li>在较稳定的基础上，引入不确定性加权或梯度平衡方法，进一步提升性能；</li>
<li>在特定阶段（如开始加入 RL 目标时）动态调整权重，避免新目标“毁掉”已有能力。</li>
</ol>
<h4 id="9143-课程学习"><a class="header" href="#9143-课程学习">9.1.4.3 课程学习</a></h4>
<p>多任务预训练之外，**课程学习（Curriculum Learning）**强调的是“<strong>任务顺序</strong>”而不是单步的损失加权。</p>
<p>直觉：先让模型学习简单的任务与样本，再逐步提高难度，可以获得更稳定、更好的训练效果。</p>
<p>在 VLA 预训练中，可以从 <strong>任务维度</strong> 和 <strong>数据维度</strong> 设计课程。</p>
<p><strong>1）按任务复杂度分阶段</strong></p>
<p>一个典型的课程可以是：</p>
<ul>
<li><strong>阶段 1：单模态预训练</strong>
视觉侧做 MAE / 对比学习，语言侧做 LM / MLM，训练出各自稳定的 backbone。</li>
<li><strong>阶段 2：图文对齐</strong>
加入 CLIP 式对比任务和 ITM，让图像与文本进入统一语义空间。</li>
<li><strong>阶段 3：加入动作预测 / 行为克隆</strong>
在共享 backbone 上添加动作解码头，开始使用机器人轨迹数据，学习“视觉 + 语言 → 动作”的映射。</li>
<li><strong>阶段 4：加入奖励或 RL 目标（第 9.3 节）</strong>
在已有策略基础上引入奖励优化，细化策略的稳定性和效率。</li>
</ul>
<p>各阶段之间也可以有重叠，例如阶段 2 后期就少量混入阶段 3 的损失，使过渡平滑。</p>
<p><strong>2）按样本难度设计数据课程</strong></p>
<p>例如：</p>
<ul>
<li>在图文对齐时，先使用“描述比较准确、文字较规范”的网络数据（如精挑的 caption），再逐渐加入噪声更大的 alt 文本；</li>
<li>在机器人数据上，先训练简单场景（物体少、布局固定），再逐步引入复杂场景（遮挡、多物体、动态干扰）；</li>
<li>在动作预测上，先用短轨迹、任务步骤清晰的数据，再加入长序列或包含失败案例的数据。</li>
</ul>
<blockquote>
<p>【图 9-6 占位：VLA 预训练课程学习时间轴示意图，展示预训练阶段如何从“单模态”逐步走向“多模态 + 动作 + RL”。】</p>
</blockquote>
<p>通过多任务预训练和课程学习的组合，我们可以将“互联网世界”中的视觉–语言知识与“机器人世界”中的物理–动作经验逐步融合，为后续模仿学习和强化学习阶段打下坚实基础。这也是当下具身基础模型研究的一个核心思路：<strong>先尽量在离线数据中把能学的都学掉，再把机器人宝贵的在线交互用在“最后那一点点差距”的弥合上。</strong></p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="Ch8.5.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M41.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.3 256 246.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"/></svg></span>
                            </a>

                            <a rel="next prefetch" href="Ch9.2.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M278.6 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L210.7 256 73.4 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"/></svg></span>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="Ch8.5.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M41.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.3 256 246.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"/></svg></span>
                    </a>

                    <a rel="next prefetch" href="Ch9.2.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M278.6 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L210.7 256 73.4 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"/></svg></span>
                    </a>
            </nav>

        </div>

        <template id=fa-eye><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M288 32c-80.8 0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7 0 24.6C17.3 304 48.6 356 95.4 399.4C142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1c3.3-7.9 3.3-16.7 0-24.6c-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144s64.5-144 144-144s144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64c-11.5 0-22.3-3-31.6-8.4c-.2 2.8-.4 5.5-.4 8.4c0 53 43 96 96 96s96-43 96-96s-43-96-96-96c-2.8 0-5.6 .1-8.4 .4c5.3 9.3 8.4 20.1 8.4 31.6z"/></svg></span></template>
        <template id=fa-eye-slash><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M38.8 5.1C28.4-3.1 13.3-1.2 5.1 9.2S-1.2 34.7 9.2 42.9l592 464c10.4 8.2 25.5 6.3 33.7-4.1s6.3-25.5-4.1-33.7L525.6 386.7c39.6-40.6 66.4-86.1 79.9-118.4c3.3-7.9 3.3-16.7 0-24.6c-14.9-35.7-46.2-87.7-93-131.1C465.5 68.8 400.8 32 320 32c-68.2 0-125 26.3-169.3 60.8L38.8 5.1zM223.1 149.5C248.6 126.2 282.7 112 320 112c79.5 0 144 64.5 144 144c0 24.9-6.3 48.3-17.4 68.7L408 294.5c5.2-11.8 8-24.8 8-38.5c0-53-43-96-96-96c-2.8 0-5.6 .1-8.4 .4c5.3 9.3 8.4 20.1 8.4 31.6c0 10.2-2.4 19.8-6.6 28.3l-90.3-70.8zm223.1 298L373 389.9c-16.4 6.5-34.3 10.1-53 10.1c-79.5 0-144-64.5-144-144c0-6.9 .5-13.6 1.4-20.2L83.1 161.5C60.3 191.2 44 220.8 34.5 243.7c-3.3 7.9-3.3 16.7 0 24.6c14.9 35.7 46.2 87.7 93 131.1C174.5 443.2 239.2 480 320 480c47.8 0 89.9-12.9 126.2-32.5z"/></svg></span></template>
        <template id=fa-copy><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M502.6 70.63l-61.25-61.25C435.4 3.371 427.2 0 418.7 0H255.1c-35.35 0-64 28.66-64 64l.0195 256C192 355.4 220.7 384 256 384h192c35.2 0 64-28.8 64-64V93.25C512 84.77 508.6 76.63 502.6 70.63zM464 320c0 8.836-7.164 16-16 16H255.1c-8.838 0-16-7.164-16-16L239.1 64.13c0-8.836 7.164-16 16-16h128L384 96c0 17.67 14.33 32 32 32h47.1V320zM272 448c0 8.836-7.164 16-16 16H63.1c-8.838 0-16-7.164-16-16L47.98 192.1c0-8.836 7.164-16 16-16H160V128H63.99c-35.35 0-64 28.65-64 64l.0098 256C.002 483.3 28.66 512 64 512h192c35.2 0 64-28.8 64-64v-32h-47.1L272 448z"/></svg></span></template>
        <template id=fa-play><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M73 39c-14.8-9.1-33.4-9.4-48.5-.9S0 62.6 0 80V432c0 17.4 9.4 33.4 24.5 41.9s33.7 8.1 48.5-.9L361 297c14.3-8.7 23-24.2 23-41s-8.7-32.2-23-41L73 39z"/></svg></span></template>
        <template id=fa-clock-rotate-left><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M75 75L41 41C25.9 25.9 0 36.6 0 57.9V168c0 13.3 10.7 24 24 24H134.1c21.4 0 32.1-25.9 17-41l-30.8-30.8C155 85.5 203 64 256 64c106 0 192 86 192 192s-86 192-192 192c-40.8 0-78.6-12.7-109.7-34.4c-14.5-10.1-34.4-6.6-44.6 7.9s-6.6 34.4 7.9 44.6C151.2 495 201.7 512 256 512c141.4 0 256-114.6 256-256S397.4 0 256 0C185.3 0 121.3 28.7 75 75zm181 53c-13.3 0-24 10.7-24 24V256c0 6.4 2.5 12.5 7 17l72 72c9.4 9.4 24.6 9.4 33.9 0s9.4-24.6 0-33.9l-65-65V152c0-13.3-10.7-24-24-24z"/></svg></span></template>



        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr-ef4e11c1.min.js"></script>
        <script src="mark-09e88c2c.min.js"></script>
        <script src="searcher-c2a407aa.js"></script>

        <script src="clipboard-1626706a.min.js"></script>
        <script src="highlight-abc7f01d.js"></script>
        <script src="book-a0b12cfe.js"></script>

        <!-- Custom JS scripts -->
        <script src="src/js/custom-links-620ec1bd.js"></script>



    </div>
    </body>
</html>
